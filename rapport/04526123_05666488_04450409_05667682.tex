\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{alltt}
\usepackage{color}
\usepackage{colortbl}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{pstricks}
\usepackage{verbatim}
\usepackage{comment}
\usepackage{framed}
\usepackage{listings}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{multirow}
\usepackage[config=altsf]{subfig}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage[plainpages=false,pdfpagelabels,hypertexnames=false]{hyperref}

%For pdf selection
\usepackage[T1]{fontenc}
\usepackage{lmodern}

%%%%% STYLE %%%%%%%
\topmargin	0in
\topskip	0in
\headheight	0in
\headsep	0in
\parindent	0in
\topsep		0in
\parskip	8pt
\floatsep	0in
%%%%%%%%%%%%%%%%%%%%

%%% SETUP HYPERLINK %%%%%
\hypersetup{
colorlinks 	= true,
linkcolor 	= black}
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%% COMMANDS %%%%%%%%
\renewcommand{\labelitemi}{$\bullet$}
\newcommand{\unit}[1]{\ \mathrm{#1}}
\newcommand{\degree}{\ensuremath{^\circ}}
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%% PAGE TITRE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\begin{center}
	\vspace{20pt}
	\large{\textsc{
		Intelligence artificielle bio-inspirée\\
	}}
	\vspace{20pt}
	\large{\textsc{
		P02
	}}
	\vfill
	\begin{tabular}{ll}
      Simon Mathieu & 04 450 409 \\
      Steven Denis & 05 667 682 \\
	  Michael Janelle-Montcalm & 04 526 123 \\
	  Martin Provencher &	05 666 488 \\
	\end{tabular}
	\vfill
	Novembre 2009 \\
	\textbf{Université de Sherbrooke}
	\vspace{20pt}
\end{center}
\clearpage
%%%%%%%%%%%%%%%%%%%%% TABLE DES MATIÈRES %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{spacing}{0.1}
\tableofcontents
\end{spacing}
\clearpage

\section{Introduction}

\section{Analyse des données} % Steven
% Similitudes entre les canaux (redondance entre 1 et 6, on garde 6)
% Valeurs des maxima et minima sont plus grandes lors d'une chute que lors 
% d'une non-chute

\section{Hypothèses simplificatrices}

\subsection{Logique floue}

\subsection{Réseau de neurones} % Mike
% Utilisation seulement des max et min permet d'identifier les chutes
Dans le cas du réseau de neurones, nous avons décidé de détecter s'il y a une chute ou non, peu importe le type de celle-ci. En effet, il s'agit du mandat de base de la problématique. Nous avons utilisé les données découpées telles que fournies dans les échantillons, donc nous nous simplifions la tâche en ne faisant pas de traitement en temps réel. De plus, nous simplifions nos données en entrée en omettant les lectures du capteur 1, étant donné qu'elles sont redondantes avec celles du capteur 6 mais moins significatives. Tel que présenté dans la section d'analyse des données, le maximum et le minimum du signal sont des informations qui semblent suffisantes pour pouvoir détecter l'occurence d'une chute.

\section{Représentation de l'information}

\subsection{Logique floue}

\subsection{Réseau de neurones} % Steven
% Schéma-bloc du système
% Extraction des caractéristiques (max, min)
% Entrées et sorties (fichiers de données, sorties du script)

\section{Mise en oeuvre}

\subsection{Logique floue}

\subsection{Réseau de neurones} % Mike
% Données d'entraînement (sujet 5)
% Description de l'évolution du réseau (époques)
% Paramètres d'entraînement (momentum, learning rate)
% Ne converge pas toujours
% – Loi d’apprentissage, nombre d’unités cachées, nombre d’unités de sortie à expérimenter ;
% – Critères d’entraînement et d’évaluation ;
% – Création des ensembles d’entraînement et de test en lien avec l’apprentissage ;
% – Critère de classification et de reconnaissance.

Afin de réaliser notre réseau de neurones, nous avons utilisé les données du sujet 5 comme données d'entraînement et les données du sujet 3 comme données de test. Nous en avons décidé ainsi puisque nous obtenions une meilleure détection avec ces ensembles. De nos échantillons, nous utilisons donc 50\%  de données de test et 50\% de données d'apprentissage.

Pour la réalisation de notre réseau de neurones, nous avons utilisé un algorithme d'entraînement avec rétroprogation de l'erreur. Il s'agit d'un algorithme d'entraînement supervisé qui vise à minimiser l'erreur sur les échantillons d'entraînement. Comme fonction d'erreur, nous avons mis la somme des carrés des différences ($sse$). Par essai et erreur, nous avons trouvé que l'utilisation de 8 unités cachées nous permet de faire converger le système la plupart du temps. Il arrive quand même quelques fois où l'entraînement n'aboutit pas à un système utilisable et nous ne faisons qu'exécuter l'algorithme une nouvelle fois lorsque celà se produit. De manière générale, nous obtenons un graphique d'entraînement qui ressemble à celui présenté à la figure \ref{fig:training_8units}. Nous avons aussi essayé d'utiliser d'autres quantités d'unités cachées, pour voir l'impact de ce paramètre sur le réseau. Nous avons découvert qu'il est possible d'utiliser jusqu'à une vingtaine de neurones. L'augmentation du nombre de neurones amène une convergence plus rapide du système vers les sorties désirées. (Voir les résultats en annexe aux figures \ref{fig:train_1_2}, \ref{fig:train_4_8} et \ref{fig:train_16_24}) L'inconvénient d'augmenter le nombre de neurones est qu'il est plus difficile de faire converger le réseau vers les résultats désirés. Pour 8 neurones, environ un essai sur 10 ne converge pas. Rendu à 24 neurones, il nous a été impossible d'obtenir un réseau utile. Pour détecter une chute, nous regardons si la valeur de la sortie de chute est plus grande que celle de la sortie de non-chute. La reconnaissance s'effectue donc avec la sortie qui a le meilleur score.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{image/training_8units.png}
    \caption{Réduction de l'erreur par entraînement (8 unités cachées)}
    \label{fig:training_8units}
\end{figure}

\subsection{Algorithme génétique}

Originalement, nous voulious utiliser le nombre de piques dans le graphique de la dérivé pour détecter les chutes.
Si le graphique d'un capteur contenait un pique, il s'agit probablement d'une chute, si il en contient aucun, il 
ne s'agît pas d'une chute. Si il en contient plusieurs, il s'agit d'une récupération de chute. 

Pour détecter un pique, nous utilisions une librairie matlab trouvé sur internet. La fonction de détection de pique
possède 4 paramètres permettant de contrôller quel points du graphique sont détectés comme étant des piques. 

Comme ces paramètre peuvents prendrent une très grand quantité de valuers, il s'agissait d'un cas intéressant ou utiliser
un algorithme génétique pour trouver les valeurs optimales. 

La première étape de la mise en oueuvre fut de manuellement se créer des données d'entrainement. Nous avons donc manuellement
observé les graphiques de la dérivé de certains des capteurs et avons estimé le nombre de pique. 

Armé de ces données, nous avons ensuite définit une fonction de pertinence qui permet de calculer la distance une données est de 
la valeur réel. 

Nous avons ensuite définit une population d'individu qui se composait d'un ensemble de valeurs représentent les paramètre des la
fonction qui trouve les piques. 

La prochaine étape consiste a faire reproduire et muter les individus de notre population pour produire la prochaine génération. Après 
observation, nous avons conclus qu'il était mieux de conserver les individus d'un génération dans la prochaine. Nous gardons donc les 
cinq individus les mieux adapter. 

Le choix des individus qui se reproduisent est fait à l'aide d'une fonction aléatoire pondéré de façon a ce que les individus possèdant
des meilleurs gènes aillent une meilleur probabilité de se reproduire. 

Le speudo code de notre algorithme est:

\begin{verbatim}
pop = InitPopulation

FOR n generation
  fitnesses = CalculateFitnesses pop
  breeders = SelectBreeders pop
  pop = Reproduce breeders
  pop = Mutate pop

\end{verbatim}


\section{Évaluation des performances}

\subsection{Logique floue}

\subsection{Réseau de neurones} % Mike
% Taux d'identification (avec explications des variations selon les paramètres)
% Performance avec les données d'entraînement, puis les données de test
% Différence des résultats selon le nombre d'unités cachées

La performance de notre algorithme avec les données d'apprentissage est parfaite. Ceci est dû au fait que nous fixons un seuil global d'erreur sur les données de test d'une valeur de 0.1. L'entraînement de notre réseau se termine lorsque la somme des carrés des erreurs de chaque sortie est sous ce seuil pour l'ensemble des données d'apprentissage, ce qui assure que toutes les données d'apprentissage sont bien identifiées.

Les réseaux de neurones que nous générons obtiennent habituellement de bons taux de détection avec les données de test. Le tableau \ref{tbl:neural_results} montre les taux de réussite, en faisant varier la taille de la couche cachée. Nous observons qu'en général, augmenter le nombre de neurones amène une meilleure détection. Le meilleur réseau que nous avons observé possédait 16 neurones et affichait un taux de reconnaissance de 96\%. Nous n'avons cependant pas opté pour celui-ci puisqu'il a fallu plusieurs essais avec notre système avant de le générer. Aussi, le réseau avec 1 seul neurone a requis plusieurs essais avant de converger. Évidemment, lorsque nous analysons la structure, il apparaît qu'en ayant moins d'unités dans la couche cachée que dans la couche de sortie, nous diminuons les chances de distinguer les sorties. Il est même surprenant que nous ayons pu générer un réseau valide avec cette configuration. Donc, après avoir analysé l'impact du nombre d'unités cachées, nous avons fixé notre forme à 8 neurones.
\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    Nombre de neurones & Reconnaissance chutes & Reconnaissance non-chutes & \% reconnaissance \\ \hline
    1 & 57\% & 100\% & 74\%  \\ \hline
    2 & 96\%  & 88\% & 93\% \\ \hline
    4 & 96\% & 88\% & 93\% \\ \hline
    8 & 93\% & 98\% & 95\% \\ \hline
    16 & 94\% & 98\% & 96\% \\ \hline
\end{tabular}
\caption{Résultats obtenus sur les données de test pour le réseau de neurone}
\label{tbl:neural_results}
\end{table}

L'autre paramètre que nous avons étudié est la quantité et la diversité des données d'apprentissage. Nous avons fait les tests précédents en utilisant les données du sujet 5 comme base pour l'apprentissage. Dans ce test, nous remarquons une meilleure détection en utilisant le sujet 5.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    Apprentissage & Reconnaissance chutes & Reconnaissance non-chutes & \% reconnaissance \\ \hline
    Sujet 3 &  &  &   \\ \hline
    Sujet 5 & 93\%  & 98\% & 95\% \\ \hline
    Sujet 5 + 40\% Sujet 3 &  &  &  \\ \hline
    60\% Sujet 5 + 60\% Sujet 3 & & & \\ \hline
\end{tabular}
\caption{Résultats obtenus en variant les données d'apprentissage, pour 8 unités cachées}
\label{tbl:neural_results}
\end{table}
\subsection{Algorithme génétique}

Dans notre cas, l'algorithme générique nous a permis de converger assé rapidement vers une solution assez optimale. 

Malheureusement, trouver le nombre de pics dans une fonction n'est pas un problème simple. La librairie que nous utilisions 
s'est avérer insufisente pour être capable de détecter les pics correctement dans les données que nous avions. 

% Martin est-ce que tu peux comléter avec les paramètre utilisé pour entrainer l'algo et le nombre de générations nécessaires.
% Tu n'as pas commiter ces infos sur git.

\section{Observations et perspectives futures}

\pagebreak
\section{Annexe}

\subsection{Variation du nombre d'unités cachées}
Voici les résultats de l'entraînement pour les différents nombres d'unités cachées. Le graphique représente la somme des carrés des différences entre le résultat obtenu et la sortie attendue.
\begin{figure}
    \centering
    \subfloat[Entraînement avec 1 neurone]{\includegraphics[scale=0.40]{image/training_1units.png} \label{fig:training_1units}}
    \hspace*{0.5in}
    \subfloat[Entraînement avec 2 neurones]{\includegraphics[scale=0.40]{image/training_2units.png} \label{fig:training_2units}}
    \caption{Entraînement avec 1 et 2 neurones}
    \label{fig:train_1_2}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Entraînement avec 4 neurones]{\includegraphics[scale=0.40]{image/training_4units.png} \label{fig:training_4units}}
    \hspace*{0.5in}
    \subfloat[Entraînement avec 8 neurones]{\includegraphics[scale=0.40]{image/training_8units.png} \label{fig:training_8units_annex}}
    \caption{Entraînement avec 4 et 8 neurones}
    \label{fig:train_4_8}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Entraînement avec 16 neurones]{\includegraphics[scale=0.40]{image/training_16units.png} \label{fig:training_16units}}
    \hspace*{0.5in}
    \subfloat[Entraînement avec 24 neurones]{\includegraphics[scale=0.40]{image/training_24units.png} \label{fig:training_24units}}
    \caption{Entraînement avec 16 et 24 neurones}
    \label{fig:train_16_24}
\end{figure}

\end{document}
